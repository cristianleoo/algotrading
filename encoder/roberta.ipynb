{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class RoBerta():\n",
    "    def __init__(self, data, labels, tokenizer, max_length):\n",
    "        self.data = None\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.days = 1\n",
    "\n",
    "    # def __len__(self):\n",
    "    #     return len(self.data)\n",
    "\n",
    "    # def __getitem__(self, idx):\n",
    "    #     example = self.data[idx]\n",
    "    #     labels = self.labels[idx]\n",
    "\n",
    "    #     encodings = []\n",
    "    #     for column_data in example:\n",
    "    #         if isinstance(column_data, str):\n",
    "    #             encoding = self.tokenizer(column_data, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
    "    #             encodings.append(encoding)\n",
    "\n",
    "    #     input_ids = torch.cat([encoding['input_ids'] for encoding in encodings], dim=1)\n",
    "    #     attention_mask = torch.cat([encoding['attention_mask'] for encoding in encodings], dim=1)\n",
    "\n",
    "    #     label_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    #     return {\n",
    "    #         'input_ids': input_ids.squeeze(),\n",
    "    #         'attention_mask': attention_mask.squeeze(),\n",
    "    #         'labels': label_tensor\n",
    "    #     }\n",
    "    \n",
    "    ############################################################\n",
    "\n",
    "    def get_data(self):\n",
    "        try:\n",
    "            #os.chdir(os.getcwd())\n",
    "            self.data = pd.read_csv(f'data/{self.ticker}_cleaned_data.csv')\n",
    "            print(f'{self.ticker} data imported. Size: {self.data.shape}')\n",
    "        except FileNotFoundError:\n",
    "            print(f'Error: File for {self.ticker} not found.')\n",
    "\n",
    "        #self.data = pd.read_csv(f'../data/{self.ticker}_cleaned_data.csv')\n",
    "        print(f'{self.ticker} size: {self.data.shape}')\n",
    "        self.data_original = self.data.copy()\n",
    "\n",
    "    ############################################################\n",
    "\n",
    "    def preprocess(self):\n",
    "        # Preprocess date column\n",
    "        if self.data is None:\n",
    "            self.get_data()\n",
    "\n",
    "        self.data['Date'] = pd.to_datetime(self.data['date'])\n",
    "        self.data.drop(['date'], axis=1, inplace=True)\n",
    "\n",
    "        self.data['month_sin'] = np.sin(2*np.pi*self.data['Date'].dt.month/12)\n",
    "        self.data['month_cos'] = np.cos(2*np.pi*self.data['Date'].dt.month/12)\n",
    "        self.data['day_of_month_sin'] = np.sin(2*np.pi*self.data['Date'].dt.day/31)\n",
    "        self.data['day_of_month_cos'] = np.cos(2*np.pi*self.data['Date'].dt.day/31)\n",
    "        self.data['day_of_week_sin'] = np.sin(2*np.pi*self.data['day']/5)\n",
    "        self.data['day_of_week_cos'] = np.cos(2*np.pi*self.data['day']/5)\n",
    "        self.data = self.data.drop('day', axis=1)\n",
    "\n",
    "        self.data['Year'] = self.data['Date'].dt.year\n",
    "        self.data['Month'] = self.data['Date'].dt.month\n",
    "        self.data['Day'] = self.data['Date'].dt.day\n",
    "\n",
    "        self.data = pd.get_dummies(self.data, columns=['Month'])  # one-hot encode month column\n",
    "\n",
    "        # set the 'date' column as the DataFrame's index\n",
    "        self.data.set_index('Date', inplace=True)\n",
    "\n",
    "        # lag the 'close_price' column by three months\n",
    "        self.data['close_price_lagged'] = self.data['close'].shift(-self.days)\n",
    "\n",
    "        # reset the index back to a column\n",
    "        self.data.reset_index(inplace=True)\n",
    "\n",
    "        # create new data as last three months of data\n",
    "        self.new_data = self.data[self.data['close_price_lagged'].isna()==True].copy().drop(['close_price_lagged'], axis=1)\n",
    "        self.data_orig_final = self.data.copy()\n",
    "        self.new_data_orig = self.new_data.copy()\n",
    "        self.data = self.data[self.data['close_price_lagged'].isna()==False].copy()\n",
    "\n",
    "        self.data = self.data.drop('Date', axis=1)\n",
    "        self.new_data = self.new_data.drop('Date', axis=1)\n",
    "\n",
    "        # scale data\n",
    "        scaler = StandardScaler()\n",
    "        self.data.iloc[:, 1:self.data.shape[1]-1] = scaler.fit_transform(self.data.iloc[:, 1:self.data.shape[1]-1])  # standardize year and day columns\n",
    "        self.new_data.iloc[:, 1:self.new_data.shape[1]-1] = scaler.fit_transform(self.new_data.iloc[:, 1:self.new_data.shape[1]-1])\n",
    "\n",
    "        self.X = self.data.drop('close_price_lagged', axis=1).values\n",
    "        self.y = self.data['close_price_lagged'].values.reshape(-1, 1)\n",
    "        self.new_data = self.new_data.values\n",
    "\n",
    "        # reshape for LSTM\n",
    "        self.X = self.X.reshape(self.X.shape[0], 1, self.X.shape[1])  # reshape to 3D array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>tic</th>\n",
       "      <th>day</th>\n",
       "      <th>macd</th>\n",
       "      <th>macds</th>\n",
       "      <th>...</th>\n",
       "      <th>rsi_14</th>\n",
       "      <th>rsi</th>\n",
       "      <th>close_50_sma</th>\n",
       "      <th>ma50</th>\n",
       "      <th>close_200_sma</th>\n",
       "      <th>ma200</th>\n",
       "      <th>vix</th>\n",
       "      <th>TLT</th>\n",
       "      <th>IEF</th>\n",
       "      <th>SHY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-05-01</td>\n",
       "      <td>93.21</td>\n",
       "      <td>95.29</td>\n",
       "      <td>92.79</td>\n",
       "      <td>89.659</td>\n",
       "      <td>31408900</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>89.659</td>\n",
       "      <td>89.659</td>\n",
       "      <td>89.659</td>\n",
       "      <td>89.659</td>\n",
       "      <td>15.49</td>\n",
       "      <td>118.41</td>\n",
       "      <td>101.35</td>\n",
       "      <td>83.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-05-02</td>\n",
       "      <td>94.99</td>\n",
       "      <td>95.17</td>\n",
       "      <td>93.19</td>\n",
       "      <td>88.252</td>\n",
       "      <td>27471000</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>88.955</td>\n",
       "      <td>88.955</td>\n",
       "      <td>88.955</td>\n",
       "      <td>88.955</td>\n",
       "      <td>15.97</td>\n",
       "      <td>118.28</td>\n",
       "      <td>101.37</td>\n",
       "      <td>83.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-05-03</td>\n",
       "      <td>92.96</td>\n",
       "      <td>94.93</td>\n",
       "      <td>92.45</td>\n",
       "      <td>88.781</td>\n",
       "      <td>31142500</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>...</td>\n",
       "      <td>28.812</td>\n",
       "      <td>28.812</td>\n",
       "      <td>88.897</td>\n",
       "      <td>88.897</td>\n",
       "      <td>88.897</td>\n",
       "      <td>88.897</td>\n",
       "      <td>15.90</td>\n",
       "      <td>118.81</td>\n",
       "      <td>101.63</td>\n",
       "      <td>83.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-05-04</td>\n",
       "      <td>93.32</td>\n",
       "      <td>95.37</td>\n",
       "      <td>92.92</td>\n",
       "      <td>89.810</td>\n",
       "      <td>22531300</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>4</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>...</td>\n",
       "      <td>55.618</td>\n",
       "      <td>55.618</td>\n",
       "      <td>89.125</td>\n",
       "      <td>89.125</td>\n",
       "      <td>89.125</td>\n",
       "      <td>89.125</td>\n",
       "      <td>14.77</td>\n",
       "      <td>118.99</td>\n",
       "      <td>101.64</td>\n",
       "      <td>83.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-05-07</td>\n",
       "      <td>95.17</td>\n",
       "      <td>96.71</td>\n",
       "      <td>95.10</td>\n",
       "      <td>90.810</td>\n",
       "      <td>24242000</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>0</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.018</td>\n",
       "      <td>...</td>\n",
       "      <td>68.170</td>\n",
       "      <td>68.170</td>\n",
       "      <td>89.462</td>\n",
       "      <td>89.462</td>\n",
       "      <td>89.462</td>\n",
       "      <td>89.462</td>\n",
       "      <td>14.75</td>\n",
       "      <td>118.80</td>\n",
       "      <td>101.59</td>\n",
       "      <td>83.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date   open   high    low   close    volume   tic  day   macd  macds  \\\n",
       "0  2018-05-01  93.21  95.29  92.79  89.659  31408900  MSFT    1  0.000  0.000   \n",
       "1  2018-05-02  94.99  95.17  93.19  88.252  27471000  MSFT    2 -0.032 -0.018   \n",
       "2  2018-05-03  92.96  94.93  92.45  88.781  31142500  MSFT    3 -0.024 -0.020   \n",
       "3  2018-05-04  93.32  95.37  92.92  89.810  22531300  MSFT    4  0.017 -0.007   \n",
       "4  2018-05-07  95.17  96.71  95.10  90.810  24242000  MSFT    0  0.079  0.018   \n",
       "\n",
       "   ...  rsi_14     rsi  close_50_sma    ma50  close_200_sma   ma200    vix  \\\n",
       "0  ...     NaN     NaN        89.659  89.659         89.659  89.659  15.49   \n",
       "1  ...   0.000   0.000        88.955  88.955         88.955  88.955  15.97   \n",
       "2  ...  28.812  28.812        88.897  88.897         88.897  88.897  15.90   \n",
       "3  ...  55.618  55.618        89.125  89.125         89.125  89.125  14.77   \n",
       "4  ...  68.170  68.170        89.462  89.462         89.462  89.462  14.75   \n",
       "\n",
       "      TLT     IEF    SHY  \n",
       "0  118.41  101.35  83.13  \n",
       "1  118.28  101.37  83.16  \n",
       "2  118.81  101.63  83.22  \n",
       "3  118.99  101.64  83.19  \n",
       "4  118.80  101.59  83.19  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "msft = pd.read_csv('/Users/cristianleo/Documents/GitHub/algotrading/data/MSFT_stock.csv')\n",
    "msft.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        88.252\n",
       "1        88.781\n",
       "2        89.810\n",
       "3        90.810\n",
       "4        90.423\n",
       "         ...   \n",
       "1255    304.830\n",
       "1256    307.260\n",
       "1257    305.560\n",
       "1258    305.410\n",
       "1259    304.400\n",
       "Name: close, Length: 1260, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lag the close column by 1 day by creating a new column 'lagged_close'\n",
    "target = msft['close'].shift(-1)[:-1]\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare your data\n",
    "text_columns = []  # List of lists, where each inner list contains text data for a column\n",
    "numeric_columns = msft.values  # List of lists, where each inner list contains numeric data for a column\n",
    "labels = target # List of labels\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "max_length = 128  # Set your desired maximum sequence length\n",
    "\n",
    "data = list(zip(text_columns, numeric_columns))\n",
    "dataset = CustomDataset(data, labels, tokenizer, max_length)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Create the model\n",
    "num_classes = 3  # Set the number of classes for classification\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=num_classes)\n",
    "\n",
    "# Define training arguments and create Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSFT data imported. Size: (1259, 34)\n",
      "MSFT size: (1259, 34)\n"
     ]
    }
   ],
   "source": [
    "from roberta import RoBerta\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "max_length = 128  # Set your desired maximum sequence length\n",
    "\n",
    "# data = list(zip(text_columns, numeric_columns))\n",
    "# dataset = CustomDataset(data, labels, tokenizer, max_length)\n",
    "# dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Create the model\n",
    "num_classes = 3  # Set the number of classes for classification\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=num_classes)\n",
    "\n",
    "# Define training arguments and create Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "roberta = RoBerta('MSFT', model, training_args)\n",
    "X, y, new_data = roberta.preprocess()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/var/folders/5j/l7cwj9sd0c1gqp3xyv7290sm0000gn/T/ipykernel_62388/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">3865714243.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: </span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">'/var/folders/5j/l7cwj9sd0c1gqp3xyv7290sm0000gn/T/ipykernel_62388/3865714243.py'</span>                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'pd'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/var/folders/5j/l7cwj9sd0c1gqp3xyv7290sm0000gn/T/ipykernel_62388/\u001b[0m\u001b[1;33m3865714243.py\u001b[0m:\u001b[94m1\u001b[0m in \u001b[92m<module>\u001b[0m     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: \u001b[0m                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m'/var/folders/5j/l7cwj9sd0c1gqp3xyv7290sm0000gn/T/ipykernel_62388/3865714243.py'\u001b[0m                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'pd'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv('/Users/cristianleo/Documents/GitHub/algotrading/data/MSFT_cleaned_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
